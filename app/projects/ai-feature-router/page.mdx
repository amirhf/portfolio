import ProjectPage from '@/components/projects/ProjectPage'
import ProjectHero from '@/components/projects/ProjectHero'
import ProjectSection from '@/components/projects/ProjectSection'
import ProjectScreenshot from '@/components/projects/ProjectScreenshot'

export const metadata = {
  title: 'Retail AI – Polyglot Search & Feature Router',
  description:
    'Advanced retail discovery architecture integrating Go for high-concurrency search, Python for ML inference, and a cost-aware AI Feature Router.',
}

<ProjectPage>
  <ProjectHero
    title="Retail AI – Polyglot Search & Feature Router"
    tagline="A unified high-performance architecture: Go for orchestration, Python for intelligence, and an AI Feature Router for cost control."
  >
    <p>
      This project demonstrates an <strong>Advanced Integrated Architecture</strong> designed for high-scale retail discovery. Rather than relying on a monolithic framework, it implements a <strong>Polyglot Systems Design</strong> where every component is optimized for its specific role.
    </p>
    <p>
      The system utilizes <strong>Go</strong> to handle high-concurrency search orchestration and connection pooling, ensuring p95 latency stays under 100ms. <strong>Python</strong> is employed strictly for the deep learning pipeline (CLIP/BLIP inference), where its ecosystem is strongest.
    </p>
    <p>
      Binding these layers together is the <strong>AI Feature Router</strong>—a logic mesh that dynamically routes user requests between edge caches, local quantized models, and cloud APIs based on complexity and cost/latency budgets.
    </p>

    <div className="mt-6 flex flex-wrap gap-3 text-sm">
      <a
        href="https://image-search-two-lovat.vercel.app/"
        target="_blank"
        rel="noreferrer"
        className="rounded-full bg-brand-sky px-4 py-1.5 font-medium text-ink-on-brand hover:bg-brand-sky/90"
      >
        View live demo
      </a>
      <a
        href="https://github.com/amirhf/imageSearch"
        target="_blank"
        rel="noreferrer"
        className="rounded-full border border-line px-4 py-1.5 font-medium text-ink hover:bg-surface-muted"
      >
        View source on GitHub
      </a>
    </div>
  </ProjectHero>

  <ProjectSection title="Tech Stack & Architectural Roles">
    <p>
      <strong>The Search Plane (Go):</strong> A high-performance gRPC/HTTP layer handling query fusion, tenant isolation, and Qdrant connection multiplexing. Optimized for I/O and concurrency.
    </p>
    <p>
      <strong>The Inference Plane (Python):</strong> Specialized workers handling VLM (Vision-Language Model) inference, image vectorization, and metadata extraction. Optimized for tensor operations.
    </p>
    <p>
      <strong>The Data Layer:</strong> <strong>Qdrant</strong> (Hybrid Search), PostgreSQL (Structured Metadata), Redis (Semantic Caching).
    </p>
    <p>
      <strong>The Logic Layer:</strong> Custom <strong>AI Feature Router</strong> implementing the "Cascade of Intelligence" pattern.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Core Architecture: The Polyglot Advantage">
   
    <p>
      In high-throughput environments, a single language often imposes trade-offs. This architecture leverages the strengths of both Go and Python to eliminate bottlenecks.
    </p>
    <ul>
      <li>
        <strong>Orchestration (Go):</strong> The search API acts as the "nervous system." It manages fan-out queries to vector stores and databases in parallel goroutines, handling thousands of concurrent user requests with minimal memory footprint.
      </li>
      <li>
        <strong>Intelligence (Python):</strong> The background workers act as the "brain." They run heavy PyTorch models (SigLIP, BLIP) to understand content. Because these are decoupled from the read-path, model latency never blocks user search queries.
      </li>
    </ul>
  </ProjectSection>

  ---

  <ProjectSection title="The AI Feature Router (Cost & Latency Control)">
    <ProjectScreenshot
      src="/images/projects/imageSearch/is-router-diagram.png"
      alt="Diagram of the AI Feature Router logic showing tiers from Edge to Cloud."
      caption="The Router Pattern: Traffic is filtered through cheaper layers first. Only 10% of complex queries reach the expensive Cloud API tier."
    />
    <p>
      Directly connecting users to LLM APIs is a recipe for unmanageable costs. [cite_start]This system implements a <strong>Multi-Tiered Routing Strategy</strong> to optimize for economic viability[cite: 71, 72].
    </p>
    <ul>
      <li>
        <strong>Tier 1 (Edge/Cache):</strong> Semantic caching checks if a similar query (Cosine Similarity > 0.95) was recently answered, returning results in &lt;10ms[cite: 101].
      </li>
      <li>
        <strong>Tier 2 (Local Models):</strong> Standard requests are routed to local, quantized Small Language Models (SLMs) running on-premise.
      </li>
      <li>
        <strong>Tier 3 (Cloud Fallback):</strong> Only when the router detects high ambiguity or low confidence does it escalate the request to a premium model (e.g., GPT-4o), ensuring high quality without the high bill[cite: 81].
      </li>
    </ul>
  </ProjectSection>

  ---

  <ProjectSection title="Hybrid Search & Multi-Tenancy">
    <p>
      <strong>Hybrid Search Fusion:</strong> Retail requires precision. Pure vector search can fail on specific product codes, while keyword search misses semantic context. [cite_start]This system implements <strong>Reciprocal Rank Fusion (RRF)</strong>, mathematically merging Dense Vectors (OpenCLIP) and Sparse Vectors (BM25) to deliver results that are both conceptually relevant and technically accurate[cite: 308].
    </p>
    <p>
      <strong>SaaS-Ready Multi-Tenancy:</strong> The architecture supports strict data isolation. Go middleware injects tenant context into every request, and Qdrant enforces payload filtering at the engine level. [cite_start]This ensures that in a shared infrastructure, Tenant A never sees Tenant B's inventory[cite: 315].
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Engineering Decisions (ADR)">
    <p>
      <strong>Why Split Go and Python?</strong>
      We avoid the "Two-Language Problem" by respecting the ecosystem boundaries. Using Go for the network-bound search layer allows us to utilize connection pooling and concurrency patterns that are difficult to tune in Python, while keeping the ML code in Python ensures we have access to the latest research models immediately.
    </p>
    <p>
      <strong>Why Qdrant?</strong>
      Qdrant was selected for its performance in high-concurrency scenarios and its first-class support for <strong>quantization</strong>. [cite_start]This allows us to serve millions of vectors from RAM on commodity hardware, significantly reducing the infrastructure overhead compared to other vector stores[cite: 311, 312].
    </p>
  </ProjectSection>
</ProjectPage>