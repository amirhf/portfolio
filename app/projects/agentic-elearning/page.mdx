import ProjectPage from '@/components/projects/ProjectPage'
import ProjectHero from '@/components/projects/ProjectHero'
import ProjectSection from '@/components/projects/ProjectSection'
import ProjectScreenshot from '@/components/projects/ProjectScreenshot'

export const metadata = {
  title: 'LearnPath – Agentic Workflow Orchestrator & Reference Architecture',
  description:
    'A production-ready reference architecture for deterministic AI agents, featuring Planner-Executor patterns, structured RAG, and Go+Python microservices.',
}

<ProjectPage>
  <ProjectHero
    title="LearnPath – Agentic Workflow Orchestrator & Reference Architecture"
    tagline="A production-ready blueprint for deterministic AI agents: Planner-Executor patterns, structured outputs, and polyglot microservices."
  >
    <p>
      LearnPath is not just a learning tool—it is a <strong>reference architecture</strong> for building reliable,
      scalable AI agents in a distributed system.
    </p>
    <p>
      It demonstrates how to solve the "black box" problem of LLMs by using the <strong>Planner-Executor pattern</strong>
      to decompose high-level goals into verifiable steps, and enforcing <strong>Structured Outputs</strong> (JSON schemas)
      to ensure the UI never breaks.
    </p>
    <p>
      The system is engineered as a polyglot microservices stack: a <strong>Go Gateway</strong> handles orchestration
      and high-concurrency traffic, while isolated <strong>Python services</strong> handle the probabilistic AI logic
      (planning, RAG, quiz generation).
    </p>
    <div className="mt-6 flex flex-wrap gap-3 text-sm">
      <a
        href="https://learning-path-designer.vercel.app/"
        target="_blank"
        rel="noreferrer"
        className="rounded-full bg-brand-sky px-4 py-1.5 font-medium text-ink-on-brand hover:bg-brand-sky/90"
      >
        View live demo
      </a>
      <a
        href="https://github.com/amirhf/learningPathDesigner"
        target="_blank"
        rel="noreferrer"
        className="rounded-full border border-line px-4 py-1.5 font-medium text-ink hover:bg-surface-muted"
      >
        View source on GitHub
      </a>
    </div>
  </ProjectHero>

  <ProjectSection title="Tech Stack & Role">
    <p>
      <strong>Stack:</strong> Go (Gin/Orchestrator), Python (FastAPI/AI Agents), Next.js (TypeScript), Qdrant (Vector
      Search), Postgres, Docker, OpenTelemetry.
    </p>
    <p>
      I architected the system to bridge the gap between "Python AI prototypes" and "Go production systems." My focus was
      on reliability patterns: idempotent orchestration, structured schema validation, and hybrid retrieval pipelines.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Architectural Patterns Implemented">
    <p>
      The system follows a strict separation of concerns, using Go for the reliability layer and Python for the intelligence layer.
    </p>

    <ProjectScreenshot
      src="/images/projects/learnpath/architecture-diagram.png"
      alt="System Architecture Diagram showing Go Gateway, Python Microservices, Qdrant, and Postgres."
      caption="High-level architecture: Go handles the 'Reliability Layer' (Auth, Rate Limits), while Python handles the 'Intelligence Layer' (Agents, RAG)."
    />

    <ul className="mt-8">
      <li>
        <strong>Planner-Executor Agent Pattern:</strong> Instead of a single "do it all" prompt, the system uses a
        dedicated <em>Planner Agent</em> to propose a curriculum, which is then validated and fleshed out by specific
        <em>Executor Agents</em>. This ensures long-horizon tasks remain coherent.
      </li>
      <li>
        <strong>Structured Outputs &amp; Determinism:</strong> To prevent UI crashes and "hallucinated" data formats,
        all AI services enforce strict JSON schemas. The Go gateway validates every response before it reaches the
        frontend, guaranteeing type safety.
      </li>
      <li>
        <strong>Advanced RAG &amp; Hybrid Search:</strong> A Qdrant-based pipeline that combines dense vector search
        with metadata filtering. This grounds every generated lesson in real documentation, providing "citations" for
        every AI claim.
      </li>
    </ul>
  </ProjectSection>

  ---

  <ProjectSection title="The User Journey (Agent Workflow)">
    <ProjectScreenshot
      src="/images/projects/learnpath/lp-1-home.png"
      alt="LearnPath home page showing the entry point for the agentic planner."
      caption="The entry point where users define high-level goals. The Go orchestrator takes this input to spin up the planning agents."
    />

    <p>
      The following sequence diagram illustrates how the system enforces determinism and grounding throughout the user's request:
    </p>

    <ProjectScreenshot
      src="/images/projects/learnpath/agent-workflow.png"
      alt="Sequence Diagram showing the Planner-Executor workflow."
      caption="Agent Workflow: The Planner decomposes the goal, the RAG service retrieves context, and the Executor synthesizes the final JSON plan."
    />

    <h3>1. Intent Analysis & Planning</h3>
    <p>
      The user sets a goal (e.g., "Master Event-Driven Systems"). The <strong>Planner Agent</strong> analyzes the time
      budget and prerequisites, generating a high-level DAG (Directed Acyclic Graph) of topics to cover.
    </p>

    <h3>2. Semantic Retrieval (RAG)</h3>
    <p>
      For each proposed topic, the <strong>Retrieval Agent</strong> queries the Qdrant index. It uses hybrid search to
      fetch the most relevant technical documentation, filtering out low-quality or irrelevant matches.
    </p>

    <h3>3. Grounded Content Generation</h3>
    <p>
      The <strong>Quiz & Lesson Agents</strong> synthesize the retrieved context into structured lesson plans. Every
      quiz question is linked to a specific source ID, ensuring the content is explainable and verifiable.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Resource Discovery Engine">
    <ProjectScreenshot
      src="/images/projects/learnpath/lp-2-search.png"
      alt="Search page showing results for “Microservice architecture” with relevance scores."
      caption="The 'Debug View' for the RAG pipeline, exposing the raw relevance scores and vector matches from Qdrant."
    />
    <p>
      This screen exposes the underlying RAG capabilities. It demonstrates how the system retrieves "ground truth" data
      before feeding it to the LLM. By using Qdrant's vector similarity, we ensure the agent has the right context window
      to generate accurate plans.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Structured Planning Interface">
    <ProjectScreenshot
      src="/images/projects/learnpath/lp-3-plan.png"
      alt="Form for creating a learning plan."
      caption="Input for the Planner Agent. Constraints like 'Hours per Week' act as system prompts to bound the agent's output."
    />
    <p>
      This form submits the "System Constraints" to the Planner. Unlike a chat bot, this follows a deterministic
      input-output contract, ensuring the generated schedule fits the user's exact mathematical constraints (e.g., 4
      weeks @ 5 hours/week).
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Observability & Progress">
    <ProjectScreenshot
      src="/images/projects/learnpath/lp-4-dashboard.png"
      alt="Dashboard summarizing learning paths."
      caption="The user's view of the state machine. Behind the scenes, Postgres tracks the completion state of every agent-generated node."
    />
    <p>
      The dashboard reflects the persistence layer. Because the "Plan" is stored as a structured relational object in
      Postgres (not just a chat log), users can track granular progress, pause, and resume their learning paths indefinitely.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Grounded Lessons & Citations">
    <ProjectScreenshot
      src="/images/projects/learnpath/lp-5-plan-details.png"
      alt="Plan details page showing curated resources."
      caption="The final output of the Executor Agent. Note how every lesson is tied to specific, clickable references."
    />
    <p>
      This view validates the "Reference Architecture" claim: the AI doesn't just talk; it cites. Every resource listed
      here was retrieved via the RAG pipeline and verified by the LLM as relevant to the specific lesson topic.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Verified Assessment Logic">
    <ProjectScreenshot
      src="/images/projects/learnpath/lp-6-quiz.png"
      alt="Quiz page with multiple-choice questions."
      caption="A demonstration of 'Grounded Generation.' These questions are generated strictly from the text of the retrieved articles."
    />
    <p>
      To solve the hallucination problem, the Quiz Agent is instructed to generate questions <em>only</em> based on the
      provided context chunks. If the RAG retrieval finds no data, the agent refuses to generate a question rather than
      inventing facts.
    </p>
  </ProjectSection>

  ---

  <ProjectSection title="Engineering Decisions (Why this Stack?)">
    <ul>
      <li>
        <strong>Go for Orchestration:</strong> I chose Go for the API Gateway to handle concurrent requests and define
        strict service contracts. It acts as the "adult in the room," ensuring that if Python services hang or fail, the
        system recovers gracefully.
      </li>
      <li>
        <strong>Python for AI Logic:</strong> Python remains the king of ML libraries. By isolating it in microservices,
        we get the best of both worlds: the vast ecosystem of LangChain/LlamaIndex and the operational stability of Go.
      </li>
      <li>
        <strong>Qdrant for Production RAG:</strong> Chosen over simple pgvector for its native support for payload
        filtering and high-performance Rust core, essential for filtering resources by "skill level" or "content type"
        in real-time.
      </li>
    </ul>
  </ProjectSection>
</ProjectPage>